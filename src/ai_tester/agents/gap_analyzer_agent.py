"""
Gap Analyzer Agent - Categorizes questions by priority and impact

This agent analyzes questions generated by the Questioner Agent and:
- Assigns priority levels (Critical, Important, Nice-to-have)
- Assesses impact on testing and development
- Provides recommendations for addressing gaps
"""

from typing import Dict, List, Any, Optional, Tuple
import json
from pydantic import BaseModel, Field
from .base_agent import BaseAgent


# Pydantic models for structured output
class PrioritizedQuestion(BaseModel):
    """Schema for a prioritized question"""
    question: str = Field(description="The question being analyzed")
    category: str = Field(description="Category of the question")
    priority: str = Field(description="Priority level: Critical, Important, or Nice-to-have")
    priority_score: int = Field(description="Priority score from 1-10", ge=1, le=10)
    impact: str = Field(description="Description of impact on testing/development")
    recommendation: str = Field(description="How to address this gap")
    rationale: str = Field(description="Why this question matters")


class Summary(BaseModel):
    """Schema for analysis summary"""
    total_questions: int = Field(description="Total number of questions analyzed")
    critical_count: int = Field(description="Number of critical questions")
    important_count: int = Field(description="Number of important questions")
    nice_to_have_count: int = Field(description="Number of nice-to-have questions")
    overall_readiness: str = Field(description="Overall readiness level: Low, Medium, or High")


class GapAnalysisResponse(BaseModel):
    """Complete response schema for gap analysis"""
    prioritized_questions: List[PrioritizedQuestion] = Field(description="List of questions with priorities and analysis")
    summary: Summary = Field(description="Summary of the analysis")


class GapAnalyzerAgent(BaseAgent):
    """
    Analyzes and prioritizes questions about Epic gaps
    """

    def __init__(self, llm):
        super().__init__(llm)

    def analyze_questions(
        self,
        questions: List[Dict[str, Any]],
        epic_data: Dict[str, Any],
        child_tickets: List[Dict[str, Any]] = None,
        use_structured_output: bool = True
    ) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
        """
        Analyze questions and assign priorities

        Args:
            questions: List of questions from Questioner Agent
            epic_data: Epic information for context
            child_tickets: List of child ticket data for additional context

        Returns:
            Tuple of (analysis result, error message)
            Analysis format:
            {
                "prioritized_questions": [
                    {
                        "question": "...",
                        "category": "...",
                        "priority": "Critical|Important|Nice-to-have",
                        "priority_score": 1-10,
                        "impact": "Description of impact",
                        "recommendation": "How to address this gap",
                        "rationale": "..."
                    }
                ],
                "summary": {
                    "total_questions": 10,
                    "critical_count": 3,
                    "important_count": 5,
                    "nice_to_have_count": 2,
                    "overall_readiness": "Low|Medium|High"
                }
            }
        """
        system_prompt = self._get_analyzer_system_prompt()
        user_prompt = self._build_analyzer_prompt(questions, epic_data, child_tickets or [])

        if use_structured_output:
            result, error = self._call_llm_structured(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                max_tokens=2500
            )

            if error:
                return None, error

            return result, None
        else:
            result, error = self._call_llm(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                max_tokens=2500
            )

            if error:
                return None, error

            # Parse JSON response
            analysis_data = self._parse_json_response(result)
            if not analysis_data or 'prioritized_questions' not in analysis_data:
                return None, "Failed to parse analysis from response"

            return analysis_data, None

    def _get_analyzer_system_prompt(self) -> str:
        """System prompt for Gap Analyzer Agent"""
        return """You are an expert Test Strategist and Risk Analyst.

Your role is to analyze questions about Epic requirements and prioritize them based on:
1. **Impact on Testing**: How much this gap affects test coverage and quality
2. **Risk Level**: Potential for defects or misunderstandings
3. **Blocking Nature**: Whether this must be resolved before proceeding
4. **Scope Impact**: How many tickets/features are affected

Priority Levels:
- **Critical (9-10)**: MUST be answered before test design. Blocks testing or high risk of defects.
  Examples: Missing acceptance criteria, undefined core functionality, unspecified integrations

- **Important (6-8)**: SHOULD be answered for comprehensive testing. Affects test quality.
  Examples: Edge cases, error handling details, performance requirements

- **Nice-to-have (1-5)**: COULD be answered for optimization. Minimal impact on core testing.
  Examples: UI polish details, optional features, minor clarifications

For each question, provide:
1. Priority level and score (1-10)
2. Impact description (what happens if this is not answered)
3. Concrete recommendation (how to address the gap)

Also assess overall readiness:
- **Low**: Multiple critical gaps, cannot proceed with confidence
- **Medium**: Some important gaps, proceed with caution
- **High**: Minor gaps only, ready to proceed

Return ONLY valid JSON in this exact format:
{
  "prioritized_questions": [
    {
      "question": "Original question",
      "category": "Original category",
      "priority": "Critical|Important|Nice-to-have",
      "priority_score": 8,
      "impact": "Impact description",
      "recommendation": "How to address",
      "rationale": "Original rationale"
    }
  ],
  "summary": {
    "total_questions": 10,
    "critical_count": 3,
    "important_count": 5,
    "nice_to_have_count": 2,
    "overall_readiness": "Medium",
    "readiness_rationale": "Why this readiness level"
  }
}

IMPORTANT DATA HANDLING:
- Focus on functional requirements and test scenarios only
- Do NOT generate, request, or repeat specific user identities (names, emails, usernames)
- Do NOT generate or request sensitive internal data (credentials, API keys, secrets)
- If input contains potentially sensitive data, reference it generically without repeating verbatim
- Prioritize test coverage and quality over metadata"""

    def _build_analyzer_prompt(
        self,
        questions: List[Dict[str, Any]],
        epic_data: Dict[str, Any],
        child_tickets: List[Dict[str, Any]]
    ) -> str:
        """Build the user prompt for gap analysis"""

        # Format questions
        questions_text = ""
        for i, q in enumerate(questions, 1):
            questions_text += f"\n{i}. **Question**: {q.get('question', 'N/A')}\n"
            questions_text += f"   **Category**: {q.get('category', 'N/A')}\n"
            questions_text += f"   **Rationale**: {q.get('rationale', 'N/A')}\n"

        # Format child tickets for context
        child_tickets_text = ""
        if child_tickets:
            child_tickets_text = f"\n**Child Tickets** ({len(child_tickets)} total):\n"
            for ticket in child_tickets[:20]:  # Limit to avoid token overflow
                child_tickets_text += f"- {ticket.get('key', 'N/A')}: {ticket.get('summary', 'N/A')}\n"
                desc = ticket.get('description', '')
                if desc:
                    child_tickets_text += f"  {desc[:200]}...\n"

        prompt = f"""Analyze and prioritize the following questions about this Epic:

**Epic**: {epic_data.get('key', 'N/A')} - {epic_data.get('summary', 'N/A')}

**Epic Description**:
{epic_data.get('description', 'No description provided')[:800]}

{child_tickets_text}

**Questions to Analyze**:
{questions_text}

For each question:
1. Assign a priority level (Critical, Important, Nice-to-have)
2. Assign a priority score (1-10)
3. Describe the impact if this question is not answered
4. Provide a recommendation for addressing the gap

Consider the child tickets when assessing priority - questions affecting multiple child tickets or core functionality should have higher priority.

Then provide an overall readiness assessment for this Epic.

Return ONLY the JSON response with your analysis."""

        return prompt

    def _call_llm_structured(
        self,
        system_prompt: str,
        user_prompt: str,
        max_tokens: int = 2500
    ) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
        """
        Call LLM with structured output using Pydantic model

        Args:
            system_prompt: System prompt defining the agent's role
            user_prompt: User prompt with specific task details
            max_tokens: Maximum tokens for the response

        Returns:
            Tuple of (result dict, error message)
        """
        try:
            result, error = self.llm.complete_json(
                system_prompt,
                user_prompt,
                max_tokens=max_tokens,
                pydantic_model=GapAnalysisResponse
            )

            if error:
                return None, error

            # Parse the JSON string response into a dict
            if isinstance(result, str):
                import json
                parsed = json.loads(result)
                return parsed, None
            else:
                # Already a dict
                return result, None

        except Exception as e:
            return None, f"{self.name} structured LLM call failed: {str(e)}"
